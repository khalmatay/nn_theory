{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb806e24",
   "metadata": {},
   "source": [
    "# Градиентный спуск своими руками\n",
    "\n",
    "* Старайтесь сделать код как можно более оптимальным. В частности, не используйте циклы в тех случаях, когда операцию можно совершить при помощи инструментов библиотек.\n",
    "\n",
    "* Можно использовать без доказательства любые результаты, встречавшиеся на лекциях или семинарах по курсу, если получение этих результатов не является вопросом задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys  \n",
    "# !{sys.executable} -m pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca02c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa83487",
   "metadata": {},
   "source": [
    "Ниже приведён базовый класс BaseLoss, который мы будем использовать для реализации всех наших лоссов. Менять его не нужно. У него есть два абстрактных метода:\n",
    "\n",
    "* Метод calc_loss, который будет принимать на вход объекты x, правильные ответы y и веса w и вычислять значения лосса\n",
    "* Метод calc_grad, который будет принимать на вход объекты x, правильные ответы y и веса w и вычислять значения градиента (вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class BaseLoss(abc.ABC):\n",
    "    \"\"\"Базовый класс лосса\"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5948bc",
   "metadata": {},
   "source": [
    "### Реализуйте класс MSELoss\n",
    "\n",
    "Он должен вычислять лосс и градиент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(BaseLoss):\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b73d7",
   "metadata": {},
   "source": [
    "Теперь мы можем создать объект MSELoss и при помощи него вычислять значение нашей функции потерь и градиенты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим объект лосса\n",
    "loss = MSELoss()\n",
    "\n",
    "# Создадим какой-то датасет\n",
    "X = np.arange(200).reshape(20, 10)\n",
    "y = np.arange(20)\n",
    "\n",
    "# Создадим какой-то вектор весов\n",
    "w = np.arange(10)\n",
    "\n",
    "# Выведем значение лосса и градиента на этом датасете с этим вектором весов\n",
    "print(loss.calc_loss(X, y, w))\n",
    "print(loss.calc_grad(X, y, w))\n",
    "\n",
    "# Проверка, что методы реализованы правильно\n",
    "assert loss.calc_loss(X, y, w) == 27410283.5, \"Метод calc_loss реализован неверно\"\n",
    "assert np.allclose(loss.calc_grad(X, y, w), np.array([1163180., 1172281., 1181382., 1190483., \n",
    "                                                      1199584., 1208685., 1217786., 1226887., \n",
    "                                                      1235988., 1245089.])), \"Метод calc_grad реализован неверно\"\n",
    "print(\"Всё верно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d569f",
   "metadata": {},
   "source": [
    "### Реализуйте функцию gradient_descent\n",
    "\n",
    "Функция должна принимать на вход начальное значение весов линейной модели w_init, матрицу объектов-признаков X, вектор правильных ответов y, объект функции потерь loss, размер шага lr и количество итераций n_iterations.\n",
    "\n",
    "Функция должна реализовывать цикл, в котором происходит шаг градиентного спуска (градиенты берутся из loss посредством вызова метода calc_grad) по формуле выше и возвращать траекторию спуска (список из новых значений весов на каждом шаге)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray, \n",
    "                        loss: BaseLoss, lr: float, n_iterations: int = 100000) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eed060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём датасет из двух переменных и реального вектора зависимости w_true\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]  \n",
    "y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
    "w_init = np.random.uniform(-2, 2, (n_features))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ab4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MSELoss()\n",
    "w_list = gradient_descent(w_init, X, y, loss, 0.01, 100)\n",
    "print(loss.calc_loss(X, y, w_list[0]))\n",
    "print(loss.calc_loss(X, y, w_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374eb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gd(w_list: Iterable, X: np.ndarray, y: np.ndarray, loss: BaseLoss):\n",
    "    \"\"\"\n",
    "    Функция для отрисовки траектории градиентного спуска\n",
    "    :param w_list: Список из объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать лосс при помощи loss.calc_loss(X, y, w)\n",
    "    \"\"\"\n",
    "    w_list = np.array(w_list)\n",
    "    meshgrid_space = np.linspace(-2, 2, 100)\n",
    "    A, B = np.meshgrid(meshgrid_space, meshgrid_space)\n",
    "\n",
    "    levels = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            w_tmp = np.array([A[i, j], B[i, j]])\n",
    "            levels[i, j] = loss.calc_loss(X, y, w_tmp)\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.title(\"GD trajectory\")\n",
    "    plt.xlabel(r'$w_1$')\n",
    "    plt.ylabel(r'$w_2$')\n",
    "    plt.xlim(w_list[:, 0].min() - 0.1, \n",
    "             w_list[:, 0].max() + 0.1)\n",
    "    plt.ylim(w_list[:, 1].min() - 0.1,\n",
    "             w_list[:, 1].max() + 0.1)\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "    # visualize the level set\n",
    "    CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
    "    CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "\n",
    "    # visualize trajectory\n",
    "    plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "    plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f389a",
   "metadata": {},
   "source": [
    "### При помощи функций gradient_descent и plot_gd нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра lr). Используйте не менее четырёх разных значений для lr.\n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметр lr влияет на поведение градиентного спуска\n",
    "\n",
    "Подсказки:\n",
    "\n",
    "* Функция gradient_descent возвращает историю весов, которую нужно подать в функцию plot_gd\n",
    "* Хорошие значения для lr могут лежать в промежутке от 0.0001 до 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94df86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3557a32",
   "metadata": {},
   "source": [
    "### *Реализуйте функцию stochastic_gradient_descent¶\n",
    "Функция должна принимать все те же параметры, что и функция gradient_descent, но ещё параметр batch_size, отвечающий за размер батча.\n",
    "\n",
    "Функция должна как и раньше реализовывать цикл, в котором происходит шаг градиентного спуска, но на каждом шаге считать градиент не по всей выборке X, а только по случайно выбранной части.\n",
    "\n",
    "Подсказка: для выбора случайной части можно использовать np.random.choice с правильным параметром size, чтобы выбрать случайные индексы, а потом проиндексировать получившимся массивом массив X:\n",
    "\n",
    "batch_indices = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "\n",
    "batch = X[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray, \n",
    "                        loss: BaseLoss, lr: float, batch_size: int, n_iterations: int = 1000) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d195d",
   "metadata": {},
   "source": [
    "### При помощи функций stochastic_gradient_descent и plot_gd нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра lr) и размера подвыборки (параметра batch_size). Используйте не менее четырёх разных значений для lr и batch_size.\n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметры lr и batch_size влияют на поведение стохастического градиентного спуска. Как отличается поведение стохастического градиентного спуска от обычного?\n",
    "\n",
    "Обратите внимание, что в нашем датасете всего 300 объектов, так что batch_size больше этого числа не будет иметь смысла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56af69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08167107",
   "metadata": {},
   "source": [
    "# Линейная регрессия\n",
    "Теперь давайте напишем наш класс для линейной регрессии. Он будет использовать интерфейс, знакомый нам из библиотеки sklearn.\n",
    "\n",
    "В методе fit мы будем подбирать веса w при помощи градиентного спуска нашим методом gradient_descent\n",
    "\n",
    "В методе predict мы будем применять нашу регрессию к датасету,\n",
    "\n",
    "### Допишите код в методах fit и predict класса LinearRegression\n",
    "\n",
    "В методе fit вам нужно как-то инициализировать веса w, применить gradient_descent и сохранить последнюю w из траектории.\n",
    "\n",
    "В методе predict вам нужно применить линейную регрессию и вернуть вектор ответов.\n",
    "\n",
    "Обратите внимание, что объект лосса передаётся в момент инициализации и хранится в self.loss. Его нужно использовать в fit для gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eba75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, loss: BaseLoss, lr: float = 0.1) -> None:\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':\n",
    "        \n",
    "        #ваш код\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        #ваш код\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a14a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4c499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_raw = pd.read_csv(\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\", \n",
    "    header=None, \n",
    "    na_values=[\"?\"]\n",
    ")\n",
    "X_raw.head()\n",
    "X_raw = X_raw[~X_raw[25].isna()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_raw[25]\n",
    "X_raw = X_raw.drop(25, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49efd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eebce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys  \n",
    "# !{sys.executable} -m pip install --user scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_raw = X_raw.drop('index', axis=1) # Удаляем ненужный столбец\n",
    "df = pd.DataFrame(data=X_raw)\n",
    "df.info() #Посмотрим где какие признаки\n",
    "df.isnull().sum() # Посчитаем пустые значения\n",
    "nans = [1, 18, 19, 21, 22]\n",
    "for i in nans:\n",
    "    df[i] = df[i].fillna(df[i].mean()) # Заменяем в вещественных признаках наны на средние\n",
    "df[5] = df[5].fillna('unknown') #заполнили пропуски и в категориальном признаке\n",
    "features = [2,3,4,5,6,7,8,14,15,17]\n",
    "categorical_features = pd.DataFrame(data=df[features]) #отделяем категориальные признаки и удаляем\n",
    "df = df.drop(features, axis=1)\n",
    "categorical_features = pd.get_dummies(categorical_features, prefix=features) # one-hot для категориальных\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "df = pd.DataFrame(data=scaler.transform(df)) # нормализуем вещ признаки\n",
    "df = df.merge(categorical_features, left_index=True, right_index=True) # вернули категориальные на место\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, y, test_size = 0.3, random_state=6)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ca8fc",
   "metadata": {},
   "source": [
    "### Обучите написанную вами линейную регрессию\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993ad24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2975b319",
   "metadata": {},
   "source": [
    "### Посчитайте ошибку обученной регрессии на обучающей и тестовой выборке при помощи метода mean_squared_error из sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82a87a",
   "metadata": {},
   "source": [
    "### Наша модель переобучилась. Давайте как обычно в такой ситуации добавим к ней L2 регуляризацию. Для этого нам нужно написать новый класс лосса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEL2Loss(BaseLoss):\n",
    "    def __init__(self, coef: float = 1.):\n",
    "        \"\"\"\n",
    "        :param coef: коэффициент регуляризации \n",
    "        \"\"\"\n",
    "        self.coef = coef\n",
    "    \n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета. Последний признак константный.\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии. Последний вес -- bias.\n",
    "        :output: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
    "        pass\n",
    "    \n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :output: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "    \n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bba6e9",
   "metadata": {},
   "source": [
    "### Обучите линейную регрессию и посмотрите что станет с лоссом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c22b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66604e8e",
   "metadata": {},
   "source": [
    "### *Попробуйте самостоятельно реализовать какой-то другой лосс и посмотреть что изменится:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30be2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
